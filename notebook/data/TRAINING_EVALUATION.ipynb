{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17d1a728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a165376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df=pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97f2cb64",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['writing score'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Independent and dependent features\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m X = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwriting score\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m y = df[\u001b[33m'\u001b[39m\u001b[33mwriting score\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\soglo\\Downloads\\mlCompleteProject\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:5588\u001b[39m, in \u001b[36mDataFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   5440\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdrop\u001b[39m(\n\u001b[32m   5441\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5442\u001b[39m     labels: IndexLabel | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5449\u001b[39m     errors: IgnoreRaise = \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5450\u001b[39m ) -> DataFrame | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5451\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5452\u001b[39m \u001b[33;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[32m   5453\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5586\u001b[39m \u001b[33;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[32m   5587\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5588\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5589\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5590\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5591\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5592\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5593\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5594\u001b[39m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5595\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5596\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\soglo\\Downloads\\mlCompleteProject\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:4807\u001b[39m, in \u001b[36mNDFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   4805\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes.items():\n\u001b[32m   4806\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4807\u001b[39m         obj = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4809\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   4810\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_inplace(obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\soglo\\Downloads\\mlCompleteProject\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:4849\u001b[39m, in \u001b[36mNDFrame._drop_axis\u001b[39m\u001b[34m(self, labels, axis, level, errors, only_slice)\u001b[39m\n\u001b[32m   4847\u001b[39m         new_axis = axis.drop(labels, level=level, errors=errors)\n\u001b[32m   4848\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4849\u001b[39m         new_axis = \u001b[43maxis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4850\u001b[39m     indexer = axis.get_indexer(new_axis)\n\u001b[32m   4852\u001b[39m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[32m   4853\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\soglo\\Downloads\\mlCompleteProject\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7136\u001b[39m, in \u001b[36mIndex.drop\u001b[39m\u001b[34m(self, labels, errors)\u001b[39m\n\u001b[32m   7134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m   7135\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors != \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m7136\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask].tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found in axis\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   7137\u001b[39m     indexer = indexer[~mask]\n\u001b[32m   7138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.delete(indexer)\n",
      "\u001b[31mKeyError\u001b[39m: \"['writing score'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Independent and dependent features\n",
    "X = df.drop(['writing score'], axis=1)\n",
    "y = df['writing score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46102af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X.select_dtypes(exclude=\"object\").columns\n",
    "cat_features = X.select_dtypes(include=\"object\").columns\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "numeric_transformer = StandardScaler()\n",
    "oh_transformer = OneHotEncoder()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"OneHotEncoder\", oh_transformer, cat_features),\n",
    "        (\"StandardScaler\", numeric_transformer, num_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5f864c",
   "metadata": {},
   "outputs": [
    {
     "ename": "_IncompleteInputError",
     "evalue": "incomplete input (748411014.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mX = preprocessor.fit_transform(X\u001b[39m\n                                    ^\n\u001b[31m_IncompleteInputError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "X = preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d60564d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d06266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separation of dataset into train and test \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.25, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504d9e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "processor=StandardScaler()\n",
    "X_train_scaled=processor.fit_transform(X_train)\n",
    "X_test_scaled=processor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3652b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e45cebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "models={\n",
    "    \"LogisticRegression\":LogisticRegression(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"RandomForest\":RandomForestClassifier(),\n",
    "    \"AdaBoost\":AdaBoostClassifier(),\n",
    "    \"GradientBoost\":GradientBoostingClassifier(),\n",
    "    \"XGBoost\":XGBClassifier(),\n",
    "    \"KNN\":KNeighborsClassifier(),\n",
    "    \"DecisionTree\":DecisionTreeClassifier(),\n",
    "}\n",
    "\n",
    "accuracy_dict= {}\n",
    "recall_dict= {}\n",
    "\n",
    "for i in range(len(list(models))):\n",
    "    model = list(models.values())[i]\n",
    "    \n",
    "    # Training\n",
    "    model.fit(X_train_scaled, y_train) \n",
    "\n",
    "    # Making Predictions\n",
    "    y_train_pred= model.predict(X_train_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)  \n",
    "\n",
    "    # Training Set Performance\n",
    "    model_train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    model_train_f1 = f1_score(y_train, y_train_pred, average=\"weighted\")\n",
    "    model_train_precision = precision_score(y_train, y_train_pred)\n",
    "    model_train_recall = recall_score(y_train, y_train_pred)\n",
    "    model_train_roc_auc_score = roc_auc_score(y_train, y_train_pred)\n",
    "\n",
    "    # Test Set Performance\n",
    "    model_test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    model_test_f1 = f1_score(y_test, y_test_pred, average=\"weighted\")\n",
    "    model_test_precision = precision_score(y_test, y_test_pred)\n",
    "    model_test_recall = recall_score(y_test, y_test_pred)\n",
    "    model_test_roc_auc_score = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "    # Format data for future plots:\n",
    "    accuracy_dict[list(models.keys())[i]] = [model_train_accuracy, model_test_accuracy]\n",
    "    \n",
    "    recall_dict[list(models.keys())[i]] = [model_train_recall, model_test_recall]\n",
    "\n",
    "\n",
    "    print(list(models.keys())[i])\n",
    "    print('Model performance for Training set')\n",
    "    print(\"-Accuracy: {:.4f}\".format(model_train_accuracy))\n",
    "    print(\"-F1 score: {:.4f}\".format(model_train_f1))\n",
    "    print(\"-Precision: {:.4f}\".format(model_train_precision))\n",
    "    print(\"-Recall: {:.4f}\".format(model_train_recall))\n",
    "    print(\"-Roc auc score: {:.4f}\".format(model_train_roc_auc_score))\n",
    "\n",
    "    print(\".\"*40)\n",
    "\n",
    "    print('Model performance for Test set')\n",
    "    print(\"-Accuracy: {:.4f}\".format(model_test_accuracy))\n",
    "    print(\"-F1 score: {:.4f}\".format(model_test_f1))\n",
    "    print(\"-Precision: {:.4f}\".format(model_test_precision))\n",
    "    print(\"-Recall: {:.4f}\".format(model_test_recall))\n",
    "    print(\"-Roc auc score: {:.4f}\".format(model_test_roc_auc_score))\n",
    "\n",
    "\n",
    "   \n",
    "    print(\"=\"*40)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d22878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Model Performance\n",
    "\n",
    "# Related with Accuracy\n",
    "train_list=[]\n",
    "for key, values in accuracy_dict.items():\n",
    "   train_list.append(values[0])\n",
    "\n",
    "test_list=[]\n",
    "for key, values in accuracy_dict.items():\n",
    "   test_list.append(values[1])\n",
    "    \n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=list(accuracy_dict.keys()), y=train_list)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Train Accuracy Score\")\n",
    "plt.title(\"Model Performance Comparison\")\n",
    "plt.show()\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=list(accuracy_dict.keys()), y=test_list)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Test Accuracy Score\")\n",
    "plt.title(\"Model Performance Comparison\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca60bef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'max_depth': [None, 1, 2, 3, 4, 5, 6, 10, 15, 20],\n",
    "    'max_features': ['auto', 5, 6, 7, 8],\n",
    "    'min_samples_split': [2, 8, 15, 20],\n",
    "    'n_estimators': [100, 200, 500, 100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a0e1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models list for Hyperparameter tuning\n",
    "randomcv_models= [\n",
    "    ('Random Forest', RandomForestClassifier(), params)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fba69a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "model_param = {}\n",
    "\n",
    "for name, model, params in randomcv_models:\n",
    "    random = RandomizedSearchCV(estimator=model, \n",
    "                                param_distributions=params,\n",
    "                                n_iter=100,\n",
    "                                cv=4,\n",
    "                                verbose=2,\n",
    "                                n_jobs=-1\n",
    "                               )\n",
    "\n",
    "    random.fit(X_train_scaled, y_train)\n",
    "    model_param[name] = random.best_params_\n",
    "\n",
    "for model_name in model_param:\n",
    "    print(f\"Best parameters for {model_name} are:\")\n",
    "    print(model_param[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12346cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "models={\n",
    "    \"RandomForest\":RandomForestClassifier(n_estimators=100, min_samples_split= 2, max_features= 7, max_depth= 20),\n",
    "}\n",
    "    \n",
    "for i in range(len(list(models))):\n",
    "    model = list(models.values())[i]\n",
    "    \n",
    "    # Training\n",
    "    model.fit(X_train_scaled, y_train) \n",
    "\n",
    "    # Making Predictions\n",
    "    y_train_pred= model.predict(X_train_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)  \n",
    "\n",
    "    # Training Set Performance\n",
    "    model_train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    model_train_f1 = f1_score(y_train, y_train_pred, average=\"weighted\")\n",
    "    model_train_precision = precision_score(y_train, y_train_pred)\n",
    "    model_train_recall = recall_score(y_train, y_train_pred)\n",
    "    model_train_roc_auc_score = roc_auc_score(y_train, y_train_pred)\n",
    "\n",
    "    # Test Set Performance\n",
    "    model_test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    model_test_f1 = f1_score(y_test, y_test_pred, average=\"weighted\")\n",
    "    model_test_precision = precision_score(y_test, y_test_pred)\n",
    "    model_test_recall = recall_score(y_test, y_test_pred)\n",
    "    model_test_roc_auc_score = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "    print(list(models.keys())[i])\n",
    "    print('Model performance for Training set')\n",
    "    print(\"-Accuracy: {:.4f}\".format(model_train_accuracy))\n",
    "    print(\"-F1 score: {:.4f}\".format(model_train_f1))\n",
    "    print(\"-Precision: {:.4f}\".format(model_train_precision))\n",
    "    print(\"-Recall: {:.4f}\".format(model_train_recall))\n",
    "    print(\"-Roc auc score: {:.4f}\".format(model_train_roc_auc_score))\n",
    "\n",
    "    print(\".\"*40)\n",
    "\n",
    "    print('Model performance for Test set')\n",
    "    print(\"-Accuracy: {:.4f}\".format(model_test_accuracy))\n",
    "    print(\"-F1 score: {:.4f}\".format(model_test_f1))\n",
    "    print(\"-Precision: {:.4f}\".format(model_test_precision))\n",
    "    print(\"-Recall: {:.4f}\".format(model_test_recall))\n",
    "    print(\"-Roc auc score: {:.4f}\".format(model_test_roc_auc_score))\n",
    "\n",
    "    print(\"=\"*40)\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
